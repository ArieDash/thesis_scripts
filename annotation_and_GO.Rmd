---
title: "R Notebook"
output: html_notebook
params:
  atm: "..\\annotation\\gill_meta_accession_trans_map_new.csv"
  de: "..\\data_analysis\\DE_lists\\CG_red_RvsC_P0.05C1_DE.csv"
  output: "..\\annotation\\gill_meta_gene_info_new.csv"
  gtm: "..\\annotation\\gill_meta_gene_trans_map.csv"
---

```{r}
library(rbioapi)
```


```{r}
#read in annotation, remove some unneeded columns and then get rid of duplicates
atm = unique(read.csv(params$atm)[-c(1,2)])
```

```{r}
#read in deg list
degs = read.csv(params$de)
names(degs)[names(degs) == "X"] = "accession_id"
```

```{r}
#join atm with degs to get origin info for degs
degs = merge(degs, atm)

#remove the "." and version number on each gene identifier for use with downstream tools
degs$accession_id = sapply(strsplit(degs$accession_id, ".", fixed = TRUE), function(x) x[1])

degs_sw = subset(degs, degs$origin == "swissprot")
degs_sw
```




```{r}
assayed_genes = sapply(strsplit(atm$accession_id, ".", fixed = TRUE), function(x) x[1])
de_genes = sapply(strsplit(degs$accession_id, ".", fixed = TRUE), function(x) x[1])
gene_vector = as.integer(assayed_genes %in% de_genes)
names(gene_vector) = assayed_genes
head(gene_vector)
```

```{r}
sw_genes = subset(atm, atm$origin == "swissprot")
sw_genes$accession_id = sapply(strsplit(sw_genes$accession_id, ".", fixed = TRUE), function(x) x[1])
```

```{r}
write.csv(sw_genes$accession_id, "C:\\Users\\Arie\\Desktop\\sw_genes.csv")
```


```{r}
#may need to fisx setting in Rstudio: go to tools>global options>Python>select Python interpreter and ensure that it is set to system Python installation
library(reticulate)
#use to install requests and pandas if needed
#py_install("pandas")
```


```{python}
import requests
import pandas
import time
import json

ids = ",".join(r.sw_genes['accession_id'])

response = requests.post('https://rest.uniprot.org/idmapping/run', data = {'ids': ids, 'from': 'UniProtKB_AC-ID', 'to': 'UniProtKB'})

print(response.text)
```

```{python}
status = requests.get('https://rest.uniprot.org/idmapping/status/'+json.loads(response.text)["jobId"])
status = json.loads(status.text)

#need to continue polling server every second until job is complete
while True:
  #check if a status was returned (when the result is ready, it will just give that, and there will be no jobStatus field)
  if('jobStatus' in  status.keys()):
    if(status['jobStatus'] != 'RUNNING'):
      #If it returned anything other than RUNNING, break out of the loop
      break
    else:
      #job is still running
      time.sleep(1)
      status = requests.get('https://rest.uniprot.org/idmapping/status/'+json.loads(response.text)["jobId"])
      status = json.loads(status.text)
  else:
    #jobStatus was not in the keys, so break out of the loop. The result was returned
    break
```

```{python}
def clean(result):
  result = result.split("\t")
  #clean up output to remove unnecessary headers
  if(len(result) > 10):
    if(result[3] != ""):
      result[3] = result[3].split(":")[1]
    if(result[10] != ""):
      result[10] = result[10].split(":")[1]
  return result
```


```{python}
payload = {'fields': 'gene_primary,protein_name,cc_function,protein_families,go_f,go_p,go_c,keyword,cc_tissue_specificity,cc_subcellular_location,cc_pathway,organism_name', 'format':'tsv'}
info = requests.get('https://rest.uniprot.org/idmapping/uniprotkb/results/stream/'+json.loads(response.text)["jobId"], params=payload)

results = info.text.split(sep = "\n")

#[1:-1] is to get rid of metadata at top and blank line at end
rows = [clean(result) for result in results[1:-1]]

#get rid of metadata at top and blank line at end
#rows = rows[1:-1]

sw_gene_info = pandas.DataFrame(rows, columns = ['accession_id','gene_name', 'protein_name', 'function_description', 'protein_families', 'go(molecular_function)', 'go(biological_process)', 'go(cellular_component)', 'keywords', 'tissue_specificity', 'subcellular_location', 'pathway', 'organism'])

print(sw_gene_info)
```


```{r}
write.csv(py$sw_gene_info, params$output)
```



```{r}
temp = merge(atm, py$sw_gene_info, by.x = "accession_id", by.y = "accession_id")
gtm = data.frame("transcript_name" = temp$accession_id, "gene_name" = temp$gene_name, "origin" = temp$origin)
to_clean = grepl(";", gtm$gene_name)
gtm$gene_name[to_clean] = unname(sapply(gtm[to_clean,]$gene_name, function(x){unlist(strsplit(x, ";", fixed = TRUE))[1]}))
gtm = gtm[-which(gtm$gene_name == ""),]
write.csv(gtm, params$gtm)
```


```{r}
nr_genes = subset(atm, atm$origin == "nr")
```


```{python}
import xml.etree.ElementTree as ET
print(r.nr_genes["gene_name"][2])
response = requests.get('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'+"epost.fcgi?db=protein&id="+r.nr_genes["gene_name"][2])
parser = ET.fromstring(response.text)
qkey = parser.find("QueryKey").text
webenv = parser.find("WebEnv").text

print(qkey)
print(webenv)

#response = requests.get('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'+"esummary.fcgi?db=protein&query_key="+qkey+"&WebEnv="+webenv)
#print(response.text)

response = requests.get('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/'+"efetch.fcgi?db=protein&query_key="+qkey+"&WebEnv="+webenv+"&rettype=xml&retmode=xml")
parser = ET.fromstring(response.text)
print(parser.find("GBSeq").find("GBSeq_definition").text)


for x in parser.find("GBSeq").find("GBSeq_feature-table").find("GBFeature").find("GBFeature_quals").findall("GBQualifier"):
  print(x.find("GBQualifier_name").text)
  print(x.find("GBQualifier_value").text)
#print(response.text)
```


```{r}
merge(degs_sw, py$sw_gene_info, by = "gene_name")
```















































































